{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Per-Fold Performance Variance\n",
    "\n",
    "Investigate why some folds perform better than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.config import initialize_notebook\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = initialize_notebook(regenerate_run_id=False)\n",
    "seed = env.configs.run['seed']\n",
    "run_cfg = env.configs.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results from Previous Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which task to analyze\n",
    "task_name = \"clinical_vs_control\"  # Change this to analyze different tasks\n",
    "\n",
    "# Load saved results\n",
    "svm_dir = env.repo_root / \"outputs\" / run_cfg['run_name'] / run_cfg['run_id'] / f\"seed_{seed}\" / \"svm\" / task_name\n",
    "with open(svm_dir / \"results.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "svm_folds = results['svm_folds']\n",
    "baseline_folds = results['baseline_folds']\n",
    "\n",
    "print(f\"Loaded results for: {task_name}\")\n",
    "print(f\"Number of folds: {len(svm_folds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Fold Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics per fold\n",
    "fold_data = []\n",
    "\n",
    "for fold_idx, (svm_fold, baseline_fold) in enumerate(zip(svm_folds, baseline_folds)):\n",
    "    fold_info = {\n",
    "        'Fold': fold_idx + 1,\n",
    "        'SVM_ROC_AUC': svm_fold['metrics'].get('roc_auc', 0),\n",
    "        'SVM_Balanced_Acc': svm_fold['metrics']['balanced_accuracy'],\n",
    "        'SVM_Accuracy': svm_fold['metrics']['accuracy'],\n",
    "        'Baseline_ROC_AUC': baseline_fold['metrics'].get('roc_auc', 0),\n",
    "        'Baseline_Balanced_Acc': baseline_fold['metrics']['balanced_accuracy'],\n",
    "        'N_Test': len(svm_fold['y_test']),\n",
    "        'N_Positive_Test': svm_fold['y_test'].sum(),\n",
    "        'N_Negative_Test': len(svm_fold['y_test']) - svm_fold['y_test'].sum(),\n",
    "        'Best_Params': str(svm_fold.get('best_params', {})),\n",
    "    }\n",
    "    fold_data.append(fold_info)\n",
    "\n",
    "df_folds = pd.DataFrame(fold_data)\n",
    "df_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance Across Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: ROC-AUC by fold\n",
    "x = df_folds['Fold']\n",
    "axes[0, 0].plot(x, df_folds['SVM_ROC_AUC'], 'o-', label='SVM', linewidth=2, markersize=8)\n",
    "axes[0, 0].plot(x, df_folds['Baseline_ROC_AUC'], 's-', label='Baseline', linewidth=2, markersize=8)\n",
    "axes[0, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Chance')\n",
    "axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "axes[0, 0].set_ylabel('ROC-AUC', fontsize=12)\n",
    "axes[0, 0].set_title('ROC-AUC by Fold', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xticks(x)\n",
    "\n",
    "# Plot 2: Balanced Accuracy by fold\n",
    "axes[0, 1].plot(x, df_folds['SVM_Balanced_Acc'], 'o-', label='SVM', linewidth=2, markersize=8)\n",
    "axes[0, 1].plot(x, df_folds['Baseline_Balanced_Acc'], 's-', label='Baseline', linewidth=2, markersize=8)\n",
    "axes[0, 1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Chance')\n",
    "axes[0, 1].set_xlabel('Fold', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Balanced Accuracy', fontsize=12)\n",
    "axes[0, 1].set_title('Balanced Accuracy by Fold', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xticks(x)\n",
    "\n",
    "# Plot 3: Test set size by fold\n",
    "axes[1, 0].bar(x - 0.2, df_folds['N_Positive_Test'], width=0.4, label='Positive (Clinical)', alpha=0.7)\n",
    "axes[1, 0].bar(x + 0.2, df_folds['N_Negative_Test'], width=0.4, label='Negative (Control)', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Fold', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Number of Subjects', fontsize=12)\n",
    "axes[1, 0].set_title('Test Set Composition by Fold', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "axes[1, 0].set_xticks(x)\n",
    "\n",
    "# Plot 4: Performance vs Test Set Imbalance\n",
    "imbalance_ratio = df_folds['N_Negative_Test'] / df_folds['N_Positive_Test']\n",
    "axes[1, 1].scatter(imbalance_ratio, df_folds['SVM_ROC_AUC'], s=100, alpha=0.6, label='SVM')\n",
    "axes[1, 1].scatter(imbalance_ratio, df_folds['Baseline_ROC_AUC'], s=100, alpha=0.6, marker='s', label='Baseline')\n",
    "for i, fold in enumerate(df_folds['Fold']):\n",
    "    axes[1, 1].annotate(f'F{fold}', (imbalance_ratio.iloc[i], df_folds['SVM_ROC_AUC'].iloc[i]), \n",
    "                       fontsize=9, ha='right')\n",
    "axes[1, 1].set_xlabel('Imbalance Ratio (Neg/Pos)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('ROC-AUC', fontsize=12)\n",
    "axes[1, 1].set_title('Performance vs Imbalance Ratio', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nImbalance Ratio per Fold:\")\n",
    "for i, ratio in enumerate(imbalance_ratio):\n",
    "    print(f\"  Fold {i+1}: 1:{ratio:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Best Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing fold\n",
    "best_fold_idx = df_folds['SVM_ROC_AUC'].idxmax()\n",
    "best_fold_num = df_folds.iloc[best_fold_idx]['Fold']\n",
    "best_roc = df_folds.iloc[best_fold_idx]['SVM_ROC_AUC']\n",
    "\n",
    "print(f\"Best Performing Fold: {int(best_fold_num)}\")\n",
    "print(f\"ROC-AUC: {best_roc:.3f}\")\n",
    "print(f\"\\nBest fold details:\")\n",
    "print(df_folds.iloc[best_fold_idx])\n",
    "\n",
    "# Compare to worst fold\n",
    "worst_fold_idx = df_folds['SVM_ROC_AUC'].idxmin()\n",
    "worst_fold_num = df_folds.iloc[worst_fold_idx]['Fold']\n",
    "worst_roc = df_folds.iloc[worst_fold_idx]['SVM_ROC_AUC']\n",
    "\n",
    "print(f\"\\n\\nWorst Performing Fold: {int(worst_fold_num)}\")\n",
    "print(f\"ROC-AUC: {worst_roc:.3f}\")\n",
    "print(f\"\\nWorst fold details:\")\n",
    "print(df_folds.iloc[worst_fold_idx])\n",
    "\n",
    "print(f\"\\n\\nPerformance Gap: {best_roc - worst_roc:.3f} ({(best_roc - worst_roc)/worst_roc * 100:.1f}% relative difference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Predictions from Best vs Worst Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Best fold\n",
    "best_fold = svm_folds[best_fold_idx]\n",
    "cm_best = confusion_matrix(best_fold['y_test'], best_fold['y_pred'])\n",
    "\n",
    "# Worst fold\n",
    "worst_fold = svm_folds[worst_fold_idx]\n",
    "cm_worst = confusion_matrix(worst_fold['y_test'], worst_fold['y_pred'])\n",
    "\n",
    "print(f\"Confusion Matrix - Best Fold ({int(best_fold_num)}):\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"                 Neg   Pos\")\n",
    "print(f\"Actual Neg      {cm_best[0,0]:4d}  {cm_best[0,1]:4d}\")\n",
    "print(f\"Actual Pos      {cm_best[1,0]:4d}  {cm_best[1,1]:4d}\")\n",
    "print(f\"\\nSensitivity (Recall): {cm_best[1,1]/(cm_best[1,0]+cm_best[1,1]):.3f}\")\n",
    "print(f\"Specificity: {cm_best[0,0]/(cm_best[0,0]+cm_best[0,1]):.3f}\")\n",
    "\n",
    "print(f\"\\n\\nConfusion Matrix - Worst Fold ({int(worst_fold_num)}):\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"                 Neg   Pos\")\n",
    "print(f\"Actual Neg      {cm_worst[0,0]:4d}  {cm_worst[0,1]:4d}\")\n",
    "print(f\"Actual Pos      {cm_worst[1,0]:4d}  {cm_worst[1,1]:4d}\")\n",
    "print(f\"\\nSensitivity (Recall): {cm_worst[1,1]/(cm_worst[1,0]+cm_worst[1,1]):.3f}\")\n",
    "print(f\"Specificity: {cm_worst[0,0]/(cm_worst[0,0]+cm_worst[0,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUMMARY STATISTICS ACROSS FOLDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nSVM ROC-AUC:\")\n",
    "print(f\"  Mean: {df_folds['SVM_ROC_AUC'].mean():.3f}\")\n",
    "print(f\"  Std:  {df_folds['SVM_ROC_AUC'].std():.3f}\")\n",
    "print(f\"  Min:  {df_folds['SVM_ROC_AUC'].min():.3f} (Fold {int(df_folds.iloc[df_folds['SVM_ROC_AUC'].idxmin()]['Fold'])})\")\n",
    "print(f\"  Max:  {df_folds['SVM_ROC_AUC'].max():.3f} (Fold {int(df_folds.iloc[df_folds['SVM_ROC_AUC'].idxmax()]['Fold'])})\")\n",
    "\n",
    "print(f\"\\nBaseline ROC-AUC:\")\n",
    "print(f\"  Mean: {df_folds['Baseline_ROC_AUC'].mean():.3f}\")\n",
    "print(f\"  Std:  {df_folds['Baseline_ROC_AUC'].std():.3f}\")\n",
    "print(f\"  Min:  {df_folds['Baseline_ROC_AUC'].min():.3f} (Fold {int(df_folds.iloc[df_folds['Baseline_ROC_AUC'].idxmin()]['Fold'])})\")\n",
    "print(f\"  Max:  {df_folds['Baseline_ROC_AUC'].max():.3f} (Fold {int(df_folds.iloc[df_folds['Baseline_ROC_AUC'].idxmax()]['Fold'])})\")\n",
    "\n",
    "print(f\"\\nHyperparameters Used:\")\n",
    "for i, row in df_folds.iterrows():\n",
    "    print(f\"  Fold {int(row['Fold'])}: {row['Best_Params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (abcd)",
   "language": "python",
   "name": "abcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
