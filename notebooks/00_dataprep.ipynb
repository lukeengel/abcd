{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Preparation Pipeline\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "**Stage 1: Environment Setup**\n",
    "- Initialize reproducible environment with fixed random seeds\n",
    "- Load and validate all configuration files\n",
    "\n",
    "**Stage 2: Data Loading & Timepoint Extraction**\n",
    "- Load and merge neuroimaging and behavioral datasets\n",
    "- Extract baseline timepoint data for cross-sectional analysis\n",
    "\n",
    "**Stage 3: Feature Engineering**\n",
    "- Apply sex coding transformations (1→male, 2→female)\n",
    "- Create anxiety group binning from t-scores (Control<59, Subclinical 60-64, Clinical 65+)\n",
    "\n",
    "**Stage 4: Quality Control**\n",
    "- Apply surface holes QC policy (≤62 defects) to remove poor-quality scans\n",
    "- Generate QC pass/fail masks and summary statistics\n",
    "\n",
    "**Stage 5: Missing Data Handling** \n",
    "- Drop columns with >30% missing values (excluding protected metadata)\n",
    "- Require complete DTI data and essential metadata\n",
    "- Generate before/after missing data summaries\n",
    "\n",
    "**Stage 6: Modeling Splits**\n",
    "- Create stratified 80/10/10 train/val/test splits\n",
    "- Stratify by anxiety groups to ensure balanced representation\n",
    "- Generate split assignments and verify stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from core.config import initialize_notebook\n",
    "\n",
    "# Pass in name of notebook, default is \"anxiety\"\n",
    "# regenerate_run_id = True will create a new run id\n",
    "env = initialize_notebook()\n",
    "configs = env.configs\n",
    "\n",
    "# Determine active research question from run config\n",
    "research_question = configs.run['run_name']  # 'anxiety' or 'psychosis'\n",
    "research_group_col = configs.data[\"columns\"][\"mapping\"][\"research_group\"]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Research Question: {research_question.upper()}\")\n",
    "print(f\"Group Column: {research_group_col}\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.pipeline import preprocess_abcd_data\n",
    "#Full pipeline\n",
    "#train, val, test = preprocess_abcd_data(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.ingest import load_and_merge\n",
    "\n",
    "df = load_and_merge(env)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.splits import timepoint_split\n",
    "\n",
    "baseline, longitudinal = timepoint_split(env, df)\n",
    "display(baseline.head())\n",
    "display(longitudinal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.transforms import recode, binning, create_comorbid_group\n",
    "\n",
    "recoded = recode(env, baseline)\n",
    "binned = binning(env, recoded)\n",
    "binned = create_comorbid_group(binned) #only for comorbid\n",
    "display(recoded.head())\n",
    "display(binned.head())\n",
    "\n",
    "print(f\"\\n{research_question.title()} Group Counts:\")\n",
    "print(binned[research_group_col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.qc import quality_control\n",
    "\n",
    "qc_df, qc_mask = quality_control(env, binned)\n",
    "display(qc_df.head())\n",
    "display(qc_mask.head())\n",
    "\n",
    "total_pass = int(qc_mask[\"qc_pass\"].sum())\n",
    "total_fail = int((~qc_mask[\"qc_pass\"]).sum())\n",
    "\n",
    "print(f\"QC pass: {total_pass}\")\n",
    "print(f\"QC fail: {total_fail}\")\n",
    "if total_fail:\n",
    "    print(\n",
    "        \"Fail reasons:\\n\"\n",
    "        + qc_mask.loc[~qc_mask[\"qc_pass\"], \"qc_reason\"].value_counts().to_string()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.missing import (\n",
    "    summarize_missing,\n",
    "    handle_missing,\n",
    ")\n",
    "\n",
    "def imaging_columns(env, df):\n",
    "    \"\"\"Get imaging columns based on config prefixes.\"\"\"\n",
    "    imaging_cfg = env.configs.data[\"columns\"][\"imaging\"]\n",
    "    cols = []\n",
    "    for cfg in imaging_cfg.values():\n",
    "        prefixes = cfg.get(\"prefixes\", [])\n",
    "        cols.extend(\n",
    "            col for col in df.columns\n",
    "            if any(col.startswith(prefix) for prefix in prefixes)\n",
    "        )\n",
    "    return sorted(set(cols))\n",
    "\n",
    "def show_missing_summary(env, df, label):\n",
    "    \"\"\"Show missing data summary for metadata and imaging columns.\"\"\"\n",
    "    meta_cols = env.configs.data[\"columns\"][\"metadata\"]\n",
    "    imaging_cols = imaging_columns(env, df)\n",
    "    \n",
    "    meta_missing = summarize_missing(env, df[meta_cols])\n",
    "    imaging_missing = summarize_missing(env, df[imaging_cols])\n",
    "    \n",
    "    print(f\"=== {label} ===\")\n",
    "    display(meta_missing.head(10))\n",
    "    display(imaging_missing.head(10))\n",
    "\n",
    "# Show before cleanup\n",
    "show_missing_summary(env, qc_df, \"Before Cleanup\")\n",
    "\n",
    "# Apply missing data handling\n",
    "clean_df = handle_missing(env, qc_df, drop_rows=True)\n",
    "\n",
    "# Show after cleanup  \n",
    "show_missing_summary(env, clean_df, \"After Cleanup\")\n",
    "\n",
    "# Print summary statistics\n",
    "rows_removed = len(qc_df) - len(clean_df)\n",
    "columns_removed = len(qc_df.columns) - len(clean_df.columns)\n",
    "\n",
    "print(f\"=== Summary ===\")\n",
    "print(f\"Total rows removed: {rows_removed:,}\")\n",
    "print(f\"Total columns removed: {columns_removed:,}\")\n",
    "print(clean_df.shape)\n",
    "#Before filtering for QC-pass participants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Complete Pipeline Impact - Dynamic based on research question\n",
    "print(f\"=== {research_question.title()} Groups: Complete Pipeline Impact ===\")\n",
    "pre_qc = binned[research_group_col].value_counts(dropna=False)\n",
    "post_qc = qc_df[qc_df['qc_pass']][research_group_col].value_counts(dropna=False)\n",
    "final = clean_df[clean_df['qc_pass']][research_group_col].value_counts(dropna=False)\n",
    "\n",
    "pipeline_impact = pd.DataFrame({\n",
    "    'Pre-QC': pre_qc,\n",
    "    'Post-QC': post_qc,\n",
    "    'Final': final,\n",
    "    'Removed QC': pre_qc - post_qc,\n",
    "    'Removed Missing': post_qc - final,\n",
    "    'Removed QC %': ((pre_qc - post_qc) / pre_qc * 100).round(2),\n",
    "    'Removed Missing %': ((post_qc - final) / post_qc * 100).round(2),\n",
    "    'Total Removed': pre_qc - final,\n",
    "    'Total Removed %': ((pre_qc - final) / pre_qc * 100).round(2)\n",
    "})\n",
    "display(pipeline_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.splits import create_modeling_splits\n",
    "\n",
    "#Filters for QC-pass participants\n",
    "train, val, test, split_map = create_modeling_splits(env, clean_df)\n",
    "display(train.head())\n",
    "print(train.shape)\n",
    "display(val.head())\n",
    "print(val.shape)\n",
    "display(test.head())\n",
    "print(test.shape)\n",
    "display(split_map.head())\n",
    "print(split_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Modeling Splits Summary ===\")\n",
    "print(f\"Train: {len(train):,}\")\n",
    "print(f\"Val  : {len(val):,}\")\n",
    "print(f\"Test : {len(test):,}\")\n",
    "print(f\"Total: {len(train) + len(val) + len(test):,}\")\n",
    "\n",
    "print(f\"\\n=== Stratification Check for {research_question.title()} Groups ===\")\n",
    "for split_name, split_df in [(\"Train\", train), (\"Val\", val), (\"Test\", test)]:\n",
    "    group_counts = split_df[research_group_col].value_counts()\n",
    "    sex_counts = split_df[\"sex_mapped\"].value_counts() \n",
    "    print(f\"{split_name}: {dict(group_counts)} | {dict(sex_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all processed data and artifacts\n",
    "from core.preprocessing.artifacts import save_processed_data, save_qc_artifacts, save_split_map, save_provenance\n",
    "\n",
    "print(f\"=== Saving Data for {env.configs.run['run_name']} run ===\")\n",
    "\n",
    "# Save datasets\n",
    "save_processed_data(\n",
    "    env,\n",
    "    baseline=clean_df[clean_df['qc_pass']].copy(),\n",
    "    baseline_preqc=clean_df,\n",
    "    train=train,\n",
    "    val=val,\n",
    "    test=test\n",
    ")\n",
    "print(clean_df.shape)\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# Save QC artifacts\n",
    "save_qc_artifacts(env, df, qc_mask)\n",
    "\n",
    "# Save split map\n",
    "save_split_map(env, split_map)\n",
    "\n",
    "# Save provenance\n",
    "save_provenance(env, qc_mask, split_map)\n",
    "\n",
    "run_path = f\"outputs/{env.configs.run['run_name']}/{env.configs.run['run_id']}/seed_{env.configs.run['seed']}\"\n",
    "print(f\"\\nAll data saved to: {run_path}\")\n",
    "print(f\"  ✓ Datasets (baseline, baseline_preqc, train, val, test)\")\n",
    "print(f\"  ✓ QC artifacts and masks\")\n",
    "print(f\"  ✓ Split assignments\")\n",
    "print(f\"  ✓ Provenance tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (abcd)",
   "language": "python",
   "name": "abcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
