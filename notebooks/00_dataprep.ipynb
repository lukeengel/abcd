{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Preparation Pipeline\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "**Stage 1: Environment Setup**\n",
    "- Initialize reproducible environment with fixed random seeds\n",
    "- Load and validate all configuration files\n",
    "\n",
    "**Stage 2: Data Loading & Timepoint Extraction**\n",
    "- Load and merge neuroimaging and behavioral datasets\n",
    "- Extract baseline timepoint data for cross-sectional analysis\n",
    "\n",
    "**Stage 3: Feature Engineering**\n",
    "- Apply sex coding transformations (1→male, 2→female)\n",
    "- Create anxiety group binning from t-scores (Control<59, Subclinical 60-64, Clinical 65+)\n",
    "\n",
    "**Stage 4: Quality Control**\n",
    "- Apply surface holes QC policy (≤62 defects) to remove poor-quality scans\n",
    "- Generate QC pass/fail masks and summary statistics\n",
    "\n",
    "**Stage 5: Missing Data Handling** \n",
    "- Drop columns with >30% missing values (excluding protected metadata)\n",
    "- Require complete DTI data and essential metadata\n",
    "- Generate before/after missing data summaries\n",
    "\n",
    "**Stage 6: Modeling Splits**\n",
    "- Create stratified 80/10/10 train/val/test splits\n",
    "- Stratify by anxiety groups to ensure balanced representation\n",
    "- Generate split assignments and verify stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from core.config import initialize_notebook\n",
    "#from core.preprocessing.pipeline import preprocess_abcd_data\n",
    "\n",
    "# Pass in name of notebook, default is \"anxiety\"\n",
    "# regenerate_run_id = True will create a new run id\n",
    "env = initialize_notebook()\n",
    "configs = env.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.pipeline import preprocess_abcd_data\n",
    "#Full pipeline\n",
    "train, val, test = preprocess_abcd_data(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.ingest import load_and_merge\n",
    "\n",
    "df = load_and_merge(env)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.splits import timepoint_split\n",
    "\n",
    "baseline, longitudinal = timepoint_split(env, df)\n",
    "display(baseline.head())\n",
    "display(longitudinal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.transforms import recode, binning\n",
    "\n",
    "recoded = recode(env, baseline)\n",
    "binned = binning(env, recoded)\n",
    "display(recoded.head())\n",
    "display(binned.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.qc import quality_control\n",
    "\n",
    "qc_df, qc_mask = quality_control(env, binned)\n",
    "display(qc_df.head())\n",
    "display(qc_mask.head())\n",
    "\n",
    "total_pass = int(qc_mask[\"qc_pass\"].sum())\n",
    "total_fail = int((~qc_mask[\"qc_pass\"]).sum())\n",
    "\n",
    "print(f\"QC pass: {total_pass}\")\n",
    "print(f\"QC fail: {total_fail}\")\n",
    "if total_fail:\n",
    "    print(\n",
    "        \"Fail reasons:\\n\"\n",
    "        + qc_mask.loc[~qc_mask[\"qc_pass\"], \"qc_reason\"].value_counts().to_string()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.missing import (\n",
    "    summarize_missing,\n",
    "    handle_missing,\n",
    ")\n",
    "\n",
    "def imaging_columns(env, df):\n",
    "    \"\"\"Get imaging columns based on config prefixes.\"\"\"\n",
    "    imaging_cfg = env.configs.data[\"columns\"][\"imaging\"]\n",
    "    cols = []\n",
    "    for cfg in imaging_cfg.values():\n",
    "        prefixes = cfg.get(\"prefixes\", [])\n",
    "        cols.extend(\n",
    "            col for col in df.columns\n",
    "            if any(col.startswith(prefix) for prefix in prefixes)\n",
    "        )\n",
    "    return sorted(set(cols))\n",
    "\n",
    "def show_missing_summary(env, df, label):\n",
    "    \"\"\"Show missing data summary for metadata and imaging columns.\"\"\"\n",
    "    meta_cols = env.configs.data[\"columns\"][\"metadata\"]\n",
    "    imaging_cols = imaging_columns(env, df)\n",
    "    \n",
    "    meta_missing = summarize_missing(env, df[meta_cols])\n",
    "    imaging_missing = summarize_missing(env, df[imaging_cols])\n",
    "    \n",
    "    print(f\"=== {label} ===\")\n",
    "    display(meta_missing.head(10))\n",
    "    display(imaging_missing.head(10))\n",
    "\n",
    "# Show before cleanup\n",
    "show_missing_summary(env, qc_df, \"Before Cleanup\")\n",
    "\n",
    "# Apply missing data handling\n",
    "clean_df = handle_missing(env, qc_df, drop_rows=True)\n",
    "\n",
    "# Show after cleanup  \n",
    "show_missing_summary(env, clean_df, \"After Cleanup\")\n",
    "\n",
    "# Print summary statistics\n",
    "rows_removed = len(qc_df) - len(clean_df)\n",
    "columns_removed = len(qc_df.columns) - len(clean_df.columns)\n",
    "\n",
    "print(f\"=== Summary ===\")\n",
    "print(f\"Total rows removed: {rows_removed:,}\")\n",
    "print(f\"Total columns removed: {columns_removed:,}\")\n",
    "print(clean_df.shape)\n",
    "#Before filtering for QC-pass participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessing.splits import create_modeling_splits\n",
    "\n",
    "#Filters for QC-pass participants\n",
    "train, val, test, split_map = create_modeling_splits(env, clean_df)\n",
    "display(train.head())\n",
    "print(train.shape)\n",
    "display(val.head())\n",
    "print(val.shape)\n",
    "display(test.head())\n",
    "print(test.shape)\n",
    "display(split_map.head())\n",
    "print(split_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Modeling Splits Summary ===\")\n",
    "print(f\"Train: {len(train):,}\")\n",
    "print(f\"Val  : {len(val):,}\")\n",
    "print(f\"Test : {len(test):,}\")\n",
    "print(f\"Total: {len(train) + len(val) + len(test):,}\")\n",
    "\n",
    "print(\"\\n=== Stratification Check ===\")\n",
    "for split_name, split_df in [(\"Train\", train), (\"Val\", val), (\"Test\", test)]:\n",
    "    anx_counts = split_df[\"anx_group\"].value_counts()\n",
    "    sex_counts = split_df[\"sex_mapped\"].value_counts() \n",
    "    print(f\"{split_name}: {dict(anx_counts)} | {dict(sex_counts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (abcd)",
   "language": "python",
   "name": "abcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
