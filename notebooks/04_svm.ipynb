{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification with Nested Cross-Validation\n",
    "\n",
    "Rigorous evaluation using:\n",
    "- Nested preprocessing (Harmonize→Scale→PCA per fold)\n",
    "- 5-fold stratified CV on 90% dev set\n",
    "- Held-out 10% test set for final evaluation\n",
    "- Baseline (Logistic Regression) comparison\n",
    "- Feature importance → brain region mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.config import initialize_notebook\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "env = initialize_notebook(regenerate_run_id=False)\n",
    "\n",
    "research_question = env.configs.run['run_name']\n",
    "seed = env.configs.run['seed']\n",
    "kernel = env.configs.svm['model']['kernel']\n",
    "\n",
    "print(f\"Research Question: {research_question.upper()}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"SVM Kernel: {kernel}\")\n",
    "print(f\"CV Folds: {env.configs.svm['cv']['n_splits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Combine train + val → 90% development set for CV.\n",
    "Keep test set (10%) completely held out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import load_development_data\n",
    "\n",
    "dev_df, data_dir = load_development_data(env)\n",
    "test_df = pd.read_parquet(data_dir / \"test.parquet\")\n",
    "\n",
    "print(f\"Development set: {len(dev_df):,} subjects\")\n",
    "print(f\"Test set: {len(test_df):,} subjects\")\n",
    "\n",
    "group_col = env.configs.data['columns']['mapping']['research_group']\n",
    "print(f\"\\nDev group distribution:\\n{dev_df[group_col].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Classification Task\n",
    "\n",
    "Test with single task first before running all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = env.configs.svm['tasks']\n",
    "print(\"Available tasks:\")\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"  {i}: {task['name']}\")\n",
    "\n",
    "# Select first task for testing\n",
    "task_config = tasks[1]\n",
    "print(f\"\\nTesting with: {task_config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Logistic Regression with Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import filter_task_data, run_nested_cv\n",
    "from core.svm.models import create_baseline\n",
    "\n",
    "# Filter data for this task\n",
    "dev_filtered, y_dev = filter_task_data(dev_df, task_config, group_col)\n",
    "test_filtered, y_test = filter_task_data(test_df, task_config, group_col)\n",
    "\n",
    "print(f\"Task: {task_config['name']}\")\n",
    "print(f\"Dev: {len(y_dev)} | Test: {len(y_test)}\")\n",
    "print(f\"Positive class ratio: {y_dev.mean():.2%}\")\n",
    "\n",
    "# Run baseline with nested CV\n",
    "baseline = create_baseline(env.configs.svm, seed)\n",
    "print(\"\\nRunning baseline with nested CV\")\n",
    "baseline_cv = run_nested_cv(dev_filtered, y_dev, baseline, env, seed, use_wandb=False)\n",
    "\n",
    "print(\"\\nBaseline CV Results:\")\n",
    "print(f\"  Accuracy: {baseline_cv['aggregated']['accuracy_mean']:.3f} ± {baseline_cv['aggregated']['accuracy_std']:.3f}\")\n",
    "print(f\"  Balanced Accuracy: {baseline_cv['aggregated']['balanced_accuracy_mean']:.3f} ± {baseline_cv['aggregated']['balanced_accuracy_std']:.3f}\")\n",
    "print(f\"  F1: {baseline_cv['aggregated']['f1_mean']:.3f} ± {baseline_cv['aggregated']['f1_std']:.3f}\")\n",
    "print(f\"  ROC-AUC: {baseline_cv['aggregated']['roc_auc_mean']:.3f} ± {baseline_cv['aggregated']['roc_auc_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.models import create_svm\n",
    "\n",
    "svm = create_svm(env.configs.svm, seed)\n",
    "print(f\"Running {kernel} SVM with nested CV...\")\n",
    "svm_cv = run_nested_cv(dev_filtered, y_dev, svm, env, seed, use_wandb=False)\n",
    "\n",
    "print(\"\\nSVM CV Results:\")\n",
    "print(f\"  Accuracy: {svm_cv['aggregated']['accuracy_mean']:.3f} ± {svm_cv['aggregated']['accuracy_std']:.3f}\")\n",
    "print(f\"  Balanced Accuracy: {svm_cv['aggregated']['balanced_accuracy_mean']:.3f} ± {svm_cv['aggregated']['balanced_accuracy_std']:.3f}\")\n",
    "print(f\"  F1: {svm_cv['aggregated']['f1_mean']:.3f} ± {svm_cv['aggregated']['f1_std']:.3f}\")\n",
    "print(f\"  ROC-AUC: {svm_cv['aggregated']['roc_auc_mean']:.3f} ± {svm_cv['aggregated']['roc_auc_std']:.3f}\")\n",
    "\n",
    "print(\"\\nBaseline vs SVM (CV):\")\n",
    "print(f\"  Accuracy: {(svm_cv['aggregated']['accuracy_mean'] - baseline_cv['aggregated']['accuracy_mean']):.3f}\")\n",
    "print(f\"  Balanced Accuracy: {(svm_cv['aggregated']['balanced_accuracy_mean'] - baseline_cv['aggregated']['balanced_accuracy_mean']):.3f}\")\n",
    "print(f\"  ROC-AUC: {(svm_cv['aggregated']['roc_auc_mean'] - baseline_cv['aggregated']['roc_auc_mean']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Models on Test Set\n",
    "\n",
    "Train on full dev set, evaluate once on held-out test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import run_final_model\n",
    "\n",
    "run_cfg = env.configs.run\n",
    "task_name = task_config['name']\n",
    "svm_dir = env.repo_root / \"outputs\" / run_cfg['run_name'] / run_cfg['run_id'] / f\"seed_{seed}\" / \"svm\" / task_name\n",
    "svm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Training final baseline on full dev set...\")\n",
    "baseline_final = run_final_model(dev_filtered, test_filtered, y_dev, y_test, \n",
    "                                 baseline, env, seed, f\"baseline_{task_name}\", svm_dir)\n",
    "\n",
    "print(\"Training final SVM on full dev set...\")\n",
    "svm_final = run_final_model(dev_filtered, test_filtered, y_dev, y_test,\n",
    "                            svm, env, seed, f\"svm_{task_name}\", svm_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS (FINAL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBaseline:\")\n",
    "for metric, value in baseline_final['test_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nSVM:\")\n",
    "for metric, value in svm_final['test_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.preprocessing import preprocess_fold\n",
    "from core.svm.evaluation import compute_confusion_matrix\n",
    "from core.svm.visualization import plot_confusion_matrix\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plots_dir = svm_dir / \"plots\"\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Preprocess test data\n",
    "_, X_test_pca, _ = preprocess_fold(dev_filtered, test_filtered, env, seed)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_final['model'].predict(X_test_pca)\n",
    "y_pred_svm = svm_final['model'].predict(X_test_pca)\n",
    "\n",
    "# Confusion matrices\n",
    "cm_baseline = compute_confusion_matrix(y_test, y_pred_baseline)\n",
    "cm_svm = compute_confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "plot_confusion_matrix(cm_baseline, [\"Negative\", \"Positive\"], f\"Baseline - {task_name}\",\n",
    "                     plots_dir / f\"cm_baseline_{task_name}.png\")\n",
    "plot_confusion_matrix(cm_svm, [\"Negative\", \"Positive\"], f\"SVM - {task_name}\",\n",
    "                     plots_dir / f\"cm_svm_{task_name}.png\")\n",
    "\n",
    "display(Image(str(plots_dir / f\"cm_baseline_{task_name}.png\")))\n",
    "display(Image(str(plots_dir / f\"cm_svm_{task_name}.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance → Brain Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.interpretation import get_feature_importance_permutation, map_pca_to_brain_regions\n",
    "from core.svm.feature_mapping import enrich_brain_regions\n",
    "from core.tsne.embeddings import get_imaging_columns\n",
    "from core.svm.visualization import plot_feature_importance\n",
    "\n",
    "n_components = svm_final['pipeline']['n_components']\n",
    "pca_features = [f\"PC{i+1}\" for i in range(n_components)]\n",
    "\n",
    "# Get test set PCA features (from svm_final)\n",
    "X_test_pca = svm_final['X_test_pca']\n",
    "\n",
    "# Get feature importance (using permutation for all kernels for consistency)\n",
    "svm_importance = get_feature_importance_permutation(\n",
    "    svm_final['model'], X_test_pca, y_test, pca_features, seed\n",
    ")\n",
    "\n",
    "print(f\"Top 10 Principal Components (by importance):\\n\")\n",
    "print(svm_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Map to brain regions\n",
    "all_imaging_cols = get_imaging_columns(dev_filtered, env.configs.svm['imaging_prefixes'])\n",
    "valid_features = svm_final['pipeline']['valid_features']\n",
    "imaging_cols = [col for i, col in enumerate(all_imaging_cols) if valid_features[i]]\n",
    "\n",
    "brain_regions = map_pca_to_brain_regions(\n",
    "    svm_importance, svm_final['pipeline']['pca'], imaging_cols,\n",
    "    top_n_components=10, top_n_features=20\n",
    ")\n",
    "\n",
    "# Add human-readable labels\n",
    "brain_regions_enriched = enrich_brain_regions(brain_regions, env)\n",
    "brain_regions_enriched.to_csv(svm_dir / \"brain_regions.csv\", index=False)\n",
    "\n",
    "# Display formatted table (no truncation)\n",
    "print(\"\\n\\n\" + \"=\"*120)\n",
    "print(f\"TOP 20 BRAIN REGIONS - {task_name.replace('_', ' ').title()}\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "display_df = brain_regions_enriched.head(20).copy()\n",
    "display_df.insert(0, 'Rank', range(1, 21))\n",
    "display_df['importance'] = display_df['importance'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Set display options to show full content\n",
    "pd.set_option('display.max_colwidth', None)  # No truncation\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Brain Region Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(brain_regions_enriched, f\"Top Brain Regions - {task_name}\",\n",
    "                       plots_dir / f\"brain_regions_{task_name}.png\", top_n=20)\n",
    "\n",
    "display(Image(str(plots_dir / f\"brain_regions_{task_name}.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Tasks (Optional)\n",
    "\n",
    "Once single task works, run complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.svm.pipeline import run_svm_pipeline\n",
    "# all_results = run_svm_pipeline(env, use_wandb=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (abcd)",
   "language": "python",
   "name": "abcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
