{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification with Nested Cross-Validation\n",
    "\n",
    "Rigorous evaluation using:\n",
    "- Nested preprocessing (Harmonize→Scale→PCA per fold)\n",
    "- 5-fold stratified CV on 90% dev set\n",
    "- Held-out 10% test set for final evaluation\n",
    "- Baseline (Logistic Regression) comparison\n",
    "- Feature importance → brain region mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.config import initialize_notebook\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "env = initialize_notebook(regenerate_run_id=False)\n",
    "\n",
    "research_question = env.configs.run['run_name']\n",
    "seed = env.configs.run['seed']\n",
    "kernel = env.configs.svm['model']['kernel']\n",
    "\n",
    "print(f\"Research Question: {research_question.upper()}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"SVM Kernel: {kernel}\")\n",
    "print(f\"CV Folds: {env.configs.svm['cv']['n_splits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Combine train + val → 90% development set for CV.\n",
    "Keep test set (10%) completely held out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import load_development_data\n",
    "\n",
    "dev_df, data_dir = load_development_data(env)\n",
    "test_df = pd.read_parquet(data_dir / \"test.parquet\")\n",
    "\n",
    "print(f\"Development set: {len(dev_df):,} subjects\")\n",
    "print(f\"Test set: {len(test_df):,} subjects\")\n",
    "\n",
    "group_col = env.configs.data['columns']['mapping']['research_group']\n",
    "print(f\"\\nDev group distribution:\\n{dev_df[group_col].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Classification Task\n",
    "\n",
    "Test with single task first before running all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = env.configs.svm['tasks']\n",
    "print(\"Available tasks:\")\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"  {i}: {task['name']}\")\n",
    "\n",
    "# Select first task for testing\n",
    "task_config = tasks[0]\n",
    "print(f\"\\nTesting with: {task_config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Logistic Regression with Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import filter_task_data, run_nested_cv\n",
    "from core.svm.models import create_baseline\n",
    "from core.svm.preprocessing import fit_pca_on_dev, apply_pca_to_fold\n",
    "\n",
    "# Filter data for this task\n",
    "dev_filtered, y_dev = filter_task_data(dev_df, task_config, group_col)\n",
    "test_filtered, y_test = filter_task_data(test_df, task_config, group_col)\n",
    "\n",
    "print(f\"Task: {task_config['name']}\")\n",
    "print(f\"Dev: {len(y_dev)} | Test: {len(y_test)}\")\n",
    "print(f\"Positive class ratio: {y_dev.mean():.2%}\")\n",
    "\n",
    "# Fit PCA once on full dev set (not per-fold - avoids \"mixing apples and pears\")\n",
    "print(\"\\nFitting PCA on full dev set...\")\n",
    "fitted_pipeline = fit_pca_on_dev(dev_filtered, env, seed)\n",
    "print(f\"PCA: {fitted_pipeline['n_components']} components, {fitted_pipeline['variance_explained']:.2%} variance explained\")\n",
    "\n",
    "# Run baseline with nested CV using pre-fitted PCA\n",
    "baseline = create_baseline(env.configs.svm, seed)\n",
    "print(\"\\nRunning baseline with nested CV (pre-fitted PCA)...\")\n",
    "baseline_cv = run_nested_cv(dev_filtered, y_dev, baseline, env, seed, fitted_pipeline, use_wandb=False)\n",
    "\n",
    "print(\"\\nBaseline CV Results:\")\n",
    "print(f\"  Accuracy: {baseline_cv['aggregated']['accuracy_mean']:.3f} ± {baseline_cv['aggregated']['accuracy_std']:.3f}\")\n",
    "print(f\"  Balanced Accuracy: {baseline_cv['aggregated']['balanced_accuracy_mean']:.3f} ± {baseline_cv['aggregated']['balanced_accuracy_std']:.3f}\")\n",
    "print(f\"  F1: {baseline_cv['aggregated']['f1_mean']:.3f} ± {baseline_cv['aggregated']['f1_std']:.3f}\")\n",
    "print(f\"  ROC-AUC: {baseline_cv['aggregated']['roc_auc_mean']:.3f} ± {baseline_cv['aggregated']['roc_auc_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.models import create_svm\n",
    "\n",
    "svm = create_svm(env.configs.svm, seed)\n",
    "print(f\"Running {kernel} SVM with nested CV (pre-fitted PCA)...\")\n",
    "svm_cv = run_nested_cv(dev_filtered, y_dev, svm, env, seed, fitted_pipeline, use_wandb=False)\n",
    "\n",
    "print(\"\\nSVM CV Results:\")\n",
    "print(f\"  Accuracy: {svm_cv['aggregated']['accuracy_mean']:.3f} ± {svm_cv['aggregated']['accuracy_std']:.3f}\")\n",
    "print(f\"  Balanced Accuracy: {svm_cv['aggregated']['balanced_accuracy_mean']:.3f} ± {svm_cv['aggregated']['balanced_accuracy_std']:.3f}\")\n",
    "print(f\"  F1: {svm_cv['aggregated']['f1_mean']:.3f} ± {svm_cv['aggregated']['f1_std']:.3f}\")\n",
    "print(f\"  ROC-AUC: {svm_cv['aggregated']['roc_auc_mean']:.3f} ± {svm_cv['aggregated']['roc_auc_std']:.3f}\")\n",
    "\n",
    "print(\"\\nBaseline vs SVM (CV):\")\n",
    "print(f\"  Accuracy: {(svm_cv['aggregated']['accuracy_mean'] - baseline_cv['aggregated']['accuracy_mean']):.3f}\")\n",
    "print(f\"  Balanced Accuracy: {(svm_cv['aggregated']['balanced_accuracy_mean'] - baseline_cv['aggregated']['balanced_accuracy_mean']):.3f}\")\n",
    "print(f\"  ROC-AUC: {(svm_cv['aggregated']['roc_auc_mean'] - baseline_cv['aggregated']['roc_auc_mean']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Models on Test Set\n",
    "\n",
    "Train on full dev set, evaluate once on held-out test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import run_final_model\n",
    "from core.svm.evaluation import bootstrap_test_metrics\n",
    "\n",
    "run_cfg = env.configs.run\n",
    "task_name = task_config['name']\n",
    "svm_dir = env.repo_root / \"outputs\" / run_cfg['run_name'] / run_cfg['run_id'] / f\"seed_{seed}\" / \"svm\" / task_name\n",
    "svm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Training final baseline on full dev set...\")\n",
    "baseline_final = run_final_model(dev_filtered, test_filtered, y_dev, y_test, \n",
    "                                 baseline, env, seed, f\"baseline_{task_name}\", svm_dir, fitted_pipeline)\n",
    "\n",
    "print(\"Training final SVM on full dev set...\")\n",
    "svm_final = run_final_model(dev_filtered, test_filtered, y_dev, y_test,\n",
    "                            svm, env, seed, f\"svm_{task_name}\", svm_dir, fitted_pipeline)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS (FINAL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBaseline:\")\n",
    "for metric, value in baseline_final['test_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nSVM:\")\n",
    "for metric, value in svm_final['test_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "# Bootstrap for confidence intervals (optimized - much faster now!)\n",
    "n_boot = env.configs.svm.get(\"evaluation\", {}).get(\"bootstrap\", {}).get(\"iterations\", 1000)\n",
    "print(f\"\\n\\nBootstrapping test set ({n_boot} iterations)...\")\n",
    "\n",
    "baseline_bootstrap = bootstrap_test_metrics(\n",
    "    baseline_final['model'], baseline_final['X_test_pca'], y_test, n_boot, seed\n",
    ")\n",
    "svm_bootstrap = bootstrap_test_metrics(\n",
    "    svm_final['model'], svm_final['X_test_pca'], y_test, n_boot, seed\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET WITH 95% CONFIDENCE INTERVALS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBaseline ROC-AUC: {baseline_bootstrap['roc_auc_mean']:.3f} \"\n",
    "      f\"[{baseline_bootstrap['roc_auc_ci_lower']:.3f}, {baseline_bootstrap['roc_auc_ci_upper']:.3f}]\")\n",
    "print(f\"SVM ROC-AUC: {svm_bootstrap['roc_auc_mean']:.3f} \"\n",
    "      f\"[{svm_bootstrap['roc_auc_ci_lower']:.3f}, {svm_bootstrap['roc_auc_ci_upper']:.3f}]\")\n",
    "\n",
    "print(f\"\\nCV ROC-AUC: {svm_cv['aggregated']['roc_auc_mean']:.3f} ± {svm_cv['aggregated']['roc_auc_std']:.3f}\")\n",
    "print(f\"Test ROC-AUC: {svm_bootstrap['roc_auc_mean']:.3f} (95% CI: [{svm_bootstrap['roc_auc_ci_lower']:.3f}, {svm_bootstrap['roc_auc_ci_upper']:.3f}])\")\n",
    "print(f\"CV-Test Gap: {svm_cv['aggregated']['roc_auc_mean'] - svm_bootstrap['roc_auc_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.preprocessing import preprocess_fold\n",
    "from core.svm.evaluation import compute_confusion_matrix\n",
    "from core.svm.visualization import plot_confusion_matrix\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plots_dir = svm_dir / \"plots\"\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Preprocess test data\n",
    "_, X_test_pca, _ = preprocess_fold(dev_filtered, test_filtered, env, seed)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_final['model'].predict(X_test_pca)\n",
    "y_pred_svm = svm_final['model'].predict(X_test_pca)\n",
    "\n",
    "# Confusion matrices\n",
    "cm_baseline = compute_confusion_matrix(y_test, y_pred_baseline)\n",
    "cm_svm = compute_confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "plot_confusion_matrix(cm_baseline, [\"Negative\", \"Positive\"], f\"Baseline - {task_name}\",\n",
    "                     plots_dir / f\"cm_baseline_{task_name}.png\")\n",
    "plot_confusion_matrix(cm_svm, [\"Negative\", \"Positive\"], f\"SVM - {task_name}\",\n",
    "                     plots_dir / f\"cm_svm_{task_name}.png\")\n",
    "\n",
    "display(Image(str(plots_dir / f\"cm_baseline_{task_name}.png\")))\n",
    "display(Image(str(plots_dir / f\"cm_svm_{task_name}.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV-Based Feature Importance → Brain Regions\n",
    "\n",
    "Uses averaged permutation importance across CV folds for robust estimates (more stable than single test set with small positive class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.interpretation import get_cv_feature_importance, map_pca_to_brain_regions\n",
    "from core.svm.feature_mapping import enrich_brain_regions\n",
    "from core.tsne.embeddings import get_imaging_columns\n",
    "from core.svm.visualization import plot_feature_importance\n",
    "\n",
    "# CV-based permutation importance (averaged across folds)\n",
    "print(\"Computing CV-based permutation importance...\\n\")\n",
    "n_repeats = env.configs.svm.get(\"interpretation\", {}).get(\"n_repeats\", 3)\n",
    "svm_importance = get_cv_feature_importance(svm_cv['fold_results'], seed, n_repeats=n_repeats)\n",
    "\n",
    "print(f\"\\nTop 10 Principal Components (by CV importance):\\n\")\n",
    "print(svm_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Map to brain regions using THE SAME fitted_pipeline PCA (consistent across all folds)\n",
    "all_imaging_cols = get_imaging_columns(dev_filtered, env.configs.svm['imaging_prefixes'])\n",
    "valid_features = fitted_pipeline['valid_features']\n",
    "imaging_cols = [col for i, col in enumerate(all_imaging_cols) if valid_features[i]]\n",
    "\n",
    "brain_regions = map_pca_to_brain_regions(\n",
    "    svm_importance, fitted_pipeline['pca'], imaging_cols,\n",
    "    top_n_components=10, top_n_features=20\n",
    ")\n",
    "\n",
    "# Add human-readable labels\n",
    "brain_regions_enriched = enrich_brain_regions(brain_regions, env)\n",
    "brain_regions_enriched.to_csv(svm_dir / \"brain_regions.csv\", index=False)\n",
    "\n",
    "# Display formatted table\n",
    "print(\"\\n\\n\" + \"=\"*120)\n",
    "print(f\"TOP 20 BRAIN REGIONS - {task_name.replace('_', ' ').title()}\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "display_df = brain_regions_enriched.head(20).copy()\n",
    "display_df.insert(0, 'Rank', range(1, 21))\n",
    "display_df['importance'] = display_df['importance'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_importance.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Brain Region Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(brain_regions_enriched, f\"Top Brain Regions - {task_name}\",\n",
    "                       plots_dir / f\"brain_regions_{task_name}.png\", top_n=20)\n",
    "\n",
    "display(Image(str(plots_dir / f\"brain_regions_{task_name}.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Tasks (Optional)\n",
    "\n",
    "Once single task works, run complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "# Use CV fold data - all folds now use same PCA so no dimension mismatch!\n",
    "fold_results = svm_cv['fold_results']\n",
    "X_dev_pca_list = []\n",
    "y_dev_list = []\n",
    "\n",
    "for fold_idx, fold in enumerate(fold_results):\n",
    "    X_dev_pca_list.append(fold['X_val_pca'])\n",
    "    y_dev_list.append(fold['y_val'])\n",
    "\n",
    "# Combine all folds to get full dev set (no truncation needed!)\n",
    "X_dev_pca = np.vstack(X_dev_pca_list)\n",
    "y_dev_combined = np.hstack(y_dev_list)\n",
    "\n",
    "# Use the fitted_pipeline PCA (same for all folds)\n",
    "pca = fitted_pipeline['pca']\n",
    "\n",
    "print(f\"Using pre-fitted PCA with {pca.n_components_} components\")\n",
    "print(f\"Dev set from CV folds: {X_dev_pca.shape}\")\n",
    "\n",
    "# Get original brain imaging features for brain region visualization\n",
    "from core.tsne.embeddings import get_imaging_columns\n",
    "all_imaging_cols = get_imaging_columns(dev_filtered, env.configs.svm['imaging_prefixes'])\n",
    "valid_features = fitted_pipeline['valid_features']\n",
    "imaging_cols = [col for i, col in enumerate(all_imaging_cols) if valid_features[i]]\n",
    "\n",
    "print(f\"Original brain features: {len(imaging_cols)}\")\n",
    "\n",
    "# Load full data dictionary for feature name lookups\n",
    "from core.svm.feature_mapping import load_data_dictionary\n",
    "data_dict = load_data_dictionary(env)\n",
    "if data_dict is not None:\n",
    "    brain_region_labels = dict(zip(data_dict['var_name'], data_dict['var_label']))\n",
    "else:\n",
    "    brain_region_labels = {}\n",
    "\n",
    "# Select feature to visualize\n",
    "viz_type = \"brain\"  # Change to \"PC\" to visualize principal component\n",
    "pc_idx = 0  # PC1 (0-indexed)\n",
    "brain_feature = \"mrisdp_191\"  # Example: top brain region from feature importance\n",
    "\n",
    "if viz_type == \"PC\":\n",
    "    if X_dev_pca.shape[1] >= pc_idx + 1:\n",
    "        feature_values = X_dev_pca[:, pc_idx]\n",
    "        feature_name = f\"PC{pc_idx + 1}\"\n",
    "        feature_label = f\"PC{pc_idx + 1}\"\n",
    "    else:\n",
    "        pc_idx = X_dev_pca.shape[1] - 1\n",
    "        feature_values = X_dev_pca[:, pc_idx]\n",
    "        feature_name = f\"PC{pc_idx + 1}\"\n",
    "        feature_label = f\"PC{pc_idx + 1}\"\n",
    "        print(f\"\\nNote: Only {X_dev_pca.shape[1]} components. Using {feature_name}.\")\n",
    "else:\n",
    "    # Visualize original brain region from raw dev data\n",
    "    if brain_feature in dev_filtered.columns:\n",
    "        feature_values = dev_filtered[brain_feature].values\n",
    "        feature_name = brain_feature\n",
    "        # Use human-readable label if available\n",
    "        if brain_feature in brain_region_labels:\n",
    "            feature_label = brain_region_labels[brain_feature]\n",
    "        else:\n",
    "            feature_label = brain_feature\n",
    "    else:\n",
    "        print(f\"Brain feature {brain_feature} not found in dev_filtered columns.\")\n",
    "        feature_values = None\n",
    "\n",
    "if feature_values is not None:\n",
    "    # Wrap long titles to prevent overlap\n",
    "    def wrap_title(text, width=60):\n",
    "        \"\"\"Wrap text to multiple lines if too long.\"\"\"\n",
    "        if len(text) <= width:\n",
    "            return text\n",
    "        return '\\n'.join(textwrap.wrap(text, width=width))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    class_labels = dev_filtered[group_col].values\n",
    "    colors_map = {'Control': '#1f77b4', 'Clinical': '#d62728', 'Subclinical': '#ff7f0e'}\n",
    "    \n",
    "    # Only plot classes that exist in filtered data\n",
    "    unique_labels = np.unique(class_labels)\n",
    "    \n",
    "    # Plot 1: Histogram/density by class\n",
    "    for label in unique_labels:\n",
    "        if label in colors_map:\n",
    "            mask = class_labels == label\n",
    "            axes[0].hist(feature_values[mask], bins=50, alpha=0.5, label=label, \n",
    "                        color=colors_map[label], density=True)\n",
    "    \n",
    "    axes[0].set_xlabel(f'{feature_name} value', fontsize=12)\n",
    "    axes[0].set_ylabel('Density', fontsize=12)\n",
    "    axes[0].set_title(wrap_title(feature_label) + '\\nDistribution by class', fontsize=11, fontweight='bold')\n",
    "    axes[0].legend(loc='best', fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Strip plot\n",
    "    class_to_y = {'Control': 0, 'Subclinical': 1, 'Clinical': 2}\n",
    "    y_positions = np.array([class_to_y.get(label, 0) for label in class_labels])\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    jitter = np.random.normal(0, 0.08, size=len(y_positions))\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label in colors_map:\n",
    "            mask = class_labels == label\n",
    "            axes[1].scatter(feature_values[mask], y_positions[mask] + jitter[mask], \n",
    "                           c=colors_map[label], label=label, alpha=0.4, s=10)\n",
    "    \n",
    "    axes[1].set_xlabel(f'{feature_name} value', fontsize=12)\n",
    "    axes[1].set_ylabel('Class', fontsize=12)\n",
    "    axes[1].set_yticks([0, 1, 2])\n",
    "    axes[1].set_yticklabels(['Control', 'Subclinical', 'Clinical'])\n",
    "    axes[1].set_title(wrap_title(feature_label) + '\\nValues across classes', fontsize=11, fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics (only for classes present in data)\n",
    "    print(f\"\\n{feature_label}\")\n",
    "    print(f\"Feature: {feature_name}\")\n",
    "    print(\"=\"*80)\n",
    "    for label in unique_labels:\n",
    "        mask = class_labels == label\n",
    "        values = feature_values[mask]\n",
    "        if len(values) > 0:\n",
    "            print(f\"{label:12s}: mean={values.mean():7.3f}, std={values.std():7.3f}, n={mask.sum():5d}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (abcd)",
   "language": "python",
   "name": "abcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
