{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical vs. Control SVM Classification with Nested Cross-Validation\n",
    "\n",
    "**Approach:**\n",
    "- 5-fold outer CV: Each fold uses 80% train, 20% test\n",
    "- **GridSearchCV for hyperparameter tuning**: Does its own 3-fold inner CV within the 80% training data\n",
    "- Each subject tested exactly once across all folds\n",
    "- Train on ALL data with class_weight='balanced' (no downsampling)\n",
    "- Aggregate predictions from all 5 folds for final metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.config import initialize_notebook\n",
    "\n",
    "env = initialize_notebook(regenerate_run_id=False)\n",
    "\n",
    "research_question = env.configs.run['run_name']\n",
    "seed = env.configs.run['seed']\n",
    "kernel = env.configs.svm['model']['kernel']\n",
    "class_weight = env.configs.svm['model']['class_weight']\n",
    "tuning_enabled = env.configs.svm.get('tuning', {}).get('enabled', False)\n",
    "\n",
    "print(f\"Research Question: {research_question.upper()}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"SVM Kernel: {kernel}\")\n",
    "print(f\"Class Weight: {class_weight}\")\n",
    "print(f\"Hyperparameter Tuning: {'ENABLED' if tuning_enabled else 'DISABLED'}\")\n",
    "print(f\"Outer CV Folds: {env.configs.svm['cv']['n_outer_splits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import load_full_dataset\n",
    "\n",
    "# Load ALL data for nested CV (no fixed holdout)\n",
    "full_df = load_full_dataset(env)\n",
    "\n",
    "print(f\"Total samples for nested CV: {len(full_df):,} subjects\")\n",
    "\n",
    "group_col = env.configs.data['columns']['mapping']['research_group']\n",
    "print(f\"\\nGroup distribution:\\n{full_df[group_col].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Single Task with Nested CV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import run_task_with_nested_cv\n",
    "\n",
    "tasks = env.configs.svm['tasks']\n",
    "print(\"Available tasks:\")\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"  {i}: {task['name']}\")\n",
    "\n",
    "# Select task to run (change index here)\n",
    "task_config = tasks[1]  # 0=any_vs_control, 1=clinical_vs_control, etc.\n",
    "print(f\"\\nRunning task: {task_config['name']}\")\n",
    "\n",
    "# Run the task with nested CV (all 5 folds)\n",
    "results = run_task_with_nested_cv(env, full_df, task_config, use_wandb=False, sweep_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics (aggregated from all 5 folds)\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL RESULTS (All 5 folds aggregated)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBaseline (Logistic Regression):\")\n",
    "for metric, value in results['baseline']['overall'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nSVM:\")\n",
    "for metric, value in results['svm']['overall'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-FOLD STATISTICS (Mean ± Std)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSVM Per-Fold:\")\n",
    "for metric, value in results['svm']['per_fold'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nTotal samples tested: {results['svm']['n_samples']}\")\n",
    "print(f\"Number of folds: {results['svm']['n_folds']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "run_cfg = env.configs.run\n",
    "task_name = task_config['name']\n",
    "plots_dir = env.repo_root / \"outputs\" / run_cfg['run_name'] / run_cfg['run_id'] / f\"seed_{seed}\" / \"svm\" / task_name / \"plots\"\n",
    "\n",
    "print(f\"Confusion matrices saved to: {plots_dir}\")\n",
    "\n",
    "# Display confusion matrices\n",
    "display(Image(str(plots_dir / f\"cm_baseline_{task_name}.png\")))\n",
    "display(Image(str(plots_dir / f\"cm_svm_{task_name}.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and Precision-Recall Curves with Operating Point\n",
    "\n",
    "Visualize where the tuned model sits on the ROC and precision-recall curves using the per-fold predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Load results\n",
    "run_cfg = env.configs.run\n",
    "task_name = task_config['name']\n",
    "MODEL = \"svm\"\n",
    "BASE_DIR = env.repo_root / \"outputs\" / run_cfg['run_name'] / run_cfg['run_id'] / f\"seed_{seed}\"\n",
    "\n",
    "with open(BASE_DIR / MODEL / task_name / \"results.pkl\", \"rb\") as f:\n",
    "    saved_results = pickle.load(f)\n",
    "\n",
    "folds = saved_results[f\"{MODEL}_folds\"]\n",
    "\n",
    "# Aggregate all fold predictions\n",
    "y_true = np.concatenate([fold[\"y_test\"] for fold in folds])\n",
    "scores = np.concatenate([fold[\"y_score\"] for fold in folds])\n",
    "\n",
    "# Get thresholds\n",
    "if 'threshold' in folds[0]:\n",
    "    thresholds = [fold[\"threshold\"] for fold in folds]\n",
    "    operating_thr = np.mean(thresholds)\n",
    "elif 'best_threshold' in folds[0]:\n",
    "    thresholds = [fold[\"best_threshold\"] for fold in folds]\n",
    "    operating_thr = np.mean(thresholds)\n",
    "else:\n",
    "    operating_thr = 0.0\n",
    "\n",
    "y_pred = (scores >= operating_thr).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# COMPREHENSIVE METRICS\n",
    "# ============================================================\n",
    "\n",
    "# Basic metrics\n",
    "roc_auc = roc_auc_score(y_true, scores)\n",
    "pr_auc = average_precision_score(y_true, scores)\n",
    "\n",
    "# Confusion matrix components\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate all metrics\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # TPR, Recall\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # TNR\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0    # PPV\n",
    "npv = tn / (tn + fn) if (tn + fn) > 0 else 0          # Negative Predictive Value\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "fdr = fp / (fp + tp) if (fp + tp) > 0 else 0          # False Discovery Rate\n",
    "\n",
    "# Advanced metrics\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "baseline_rate = np.sum(y_true) / len(y_true)\n",
    "lift = precision / baseline_rate if baseline_rate > 0 else 0\n",
    "nns = 1 / precision if precision > 0 else np.inf      # Number Needed to Screen\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"COMPREHENSIVE METRICS - {MODEL.upper()} - {task_name}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOperating Threshold: {operating_thr:.3f}\")\n",
    "print(f\"\\n--- Discrimination Metrics ---\")\n",
    "print(f\"ROC-AUC:              {roc_auc:.3f}\")\n",
    "print(f\"PR-AUC:               {pr_auc:.3f}\")\n",
    "print(f\"Matthews Corr Coef:   {mcc:.3f}  {'[Weak]' if mcc < 0.3 else '[Moderate]' if mcc < 0.5 else '[Strong]'}\")\n",
    "\n",
    "print(f\"\\n--- Classification Performance ---\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.3f}  (catching {100*sensitivity:.1f}% of positives)\")\n",
    "print(f\"Specificity:          {specificity:.3f}  (correctly rejecting {100*specificity:.1f}% of negatives)\")\n",
    "print(f\"Precision (PPV):      {precision:.3f}  ({100*precision:.1f}% of predictions are correct)\")\n",
    "print(f\"NPV:                  {npv:.3f}  ({100*npv:.1f}% of negative predictions are correct)\")\n",
    "\n",
    "print(f\"\\n--- Error Rates ---\")\n",
    "print(f\"False Positive Rate:  {fpr:.3f}  ({100*fpr:.1f}% of controls misclassified)\")\n",
    "print(f\"False Negative Rate:  {fnr:.3f}  (missing {100*fnr:.1f}% of positives)\")\n",
    "print(f\"False Discovery Rate: {fdr:.3f}  ({100*fdr:.1f}% of predictions are false alarms)\")\n",
    "\n",
    "print(f\"\\n--- Utility ---\")\n",
    "print(f\"Lift over Baseline:   {lift:.2f}x  (model is {lift:.1f}x better than random)\")\n",
    "print(f\"Number Needed Screen: {nns:.1f}  (examine {nns:.0f} flagged cases to find 1 true positive)\")\n",
    "\n",
    "print(f\"\\n--- Prediction Distribution ---\")\n",
    "print(f\"Baseline Positive Rate:  {100*baseline_rate:.1f}%  ({int(np.sum(y_true))}/{len(y_true)} actual positives)\")\n",
    "print(f\"Predicted Positive Rate: {100*np.sum(y_pred)/len(y_pred):.1f}%  ({np.sum(y_pred)}/{len(y_pred)} predicted)\")\n",
    "\n",
    "print(f\"\\n--- Confusion Matrix ---\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"                Neg    Pos\")\n",
    "print(f\"Actual  Neg    {tn:4d}   {fp:4d}\")\n",
    "print(f\"        Pos    {fn:4d}   {tp:4d}\")\n",
    "\n",
    "# ============================================================\n",
    "# TOP-K PRECISION CURVE\n",
    "# ============================================================\n",
    "\n",
    "# Sort predictions by confidence (highest scores first)\n",
    "sorted_idx = np.argsort(scores)[::-1]\n",
    "sorted_y_true = y_true[sorted_idx]\n",
    "\n",
    "# Calculate precision at different K values\n",
    "k_values = []\n",
    "precisions_at_k = []\n",
    "for k in range(10, min(500, len(scores)), 10):\n",
    "    k_values.append(k)\n",
    "    prec_k = np.sum(sorted_y_true[:k]) / k\n",
    "    precisions_at_k.append(prec_k)\n",
    "\n",
    "# ============================================================\n",
    "# CALIBRATION\n",
    "# ============================================================\n",
    "\n",
    "# For decision_function, map scores to [0,1] for calibration plot\n",
    "from scipy.special import expit\n",
    "probs = expit(scores)  # Sigmoid to approximate probabilities\n",
    "prob_true, prob_pred = calibration_curve(y_true, probs, n_bins=10, strategy='quantile')\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. ROC Curve\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "fpr_curve, tpr_curve, _ = roc_curve(y_true, scores)\n",
    "op_tpr = sensitivity\n",
    "op_fpr = fpr\n",
    "ax1.plot(fpr_curve, tpr_curve, label=f\"ROC (AUC={roc_auc:.3f})\", linewidth=2)\n",
    "ax1.scatter(op_fpr, op_tpr, color=\"red\", s=100, zorder=5, \n",
    "           label=f\"Operating @ {operating_thr:.2f}\")\n",
    "ax1.plot([0, 1], [0, 1], \"k--\", alpha=0.3, label=\"Random\")\n",
    "ax1.set_xlabel(\"False Positive Rate\", fontsize=11)\n",
    "ax1.set_ylabel(\"True Positive Rate (Recall)\", fontsize=11)\n",
    "ax1.set_title(f\"ROC Curve - {MODEL.upper()}\", fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "prec_curve, rec_curve, _ = precision_recall_curve(y_true, scores)\n",
    "ax2.plot(rec_curve, prec_curve, label=f\"PR (AUC={pr_auc:.3f})\", linewidth=2)\n",
    "ax2.scatter(sensitivity, precision, color=\"red\", s=100, zorder=5,\n",
    "           label=f\"Operating @ {operating_thr:.2f}\")\n",
    "ax2.axhline(baseline_rate, color='k', linestyle='--', alpha=0.3, \n",
    "           label=f\"Baseline ({baseline_rate:.3f})\")\n",
    "ax2.set_xlabel(\"Recall (Sensitivity)\", fontsize=11)\n",
    "ax2.set_ylabel(\"Precision (PPV)\", fontsize=11)\n",
    "ax2.set_title(f\"Precision-Recall Curve - {MODEL.upper()}\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.legend(loc=\"best\", fontsize=9)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Top-K Precision\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.plot(k_values, precisions_at_k, linewidth=2, color='#2ca02c')\n",
    "ax3.axhline(baseline_rate, color='k', linestyle='--', alpha=0.3, label='Baseline')\n",
    "ax3.axhline(precision, color='red', linestyle=':', alpha=0.5, \n",
    "           label=f'Operating precision ({precision:.3f})')\n",
    "ax3.set_xlabel(\"Top K Predictions\", fontsize=11)\n",
    "ax3.set_ylabel(\"Precision in Top K\", fontsize=11)\n",
    "ax3.set_title(\"Top-K Precision Curve\", fontsize=12, fontweight='bold')\n",
    "ax3.legend(loc=\"best\", fontsize=9)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Calibration Plot\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Model')\n",
    "ax4.plot([0, 1], [0, 1], \"k--\", alpha=0.3, label=\"Perfect calibration\")\n",
    "ax4.set_xlabel(\"Mean Predicted Probability\", fontsize=11)\n",
    "ax4.set_ylabel(\"Fraction of Positives\", fontsize=11)\n",
    "ax4.set_title(\"Calibration Curve\", fontsize=12, fontweight='bold')\n",
    "ax4.legend(loc=\"best\", fontsize=9)\n",
    "ax4.grid(alpha=0.3)\n",
    "ax4.set_xlim([0, 1])\n",
    "ax4.set_ylim([0, 1])\n",
    "\n",
    "# 5. Score Distribution\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.hist(scores[y_true == 0], bins=50, alpha=0.5, label='Negative', color='blue', density=True)\n",
    "ax5.hist(scores[y_true == 1], bins=50, alpha=0.5, label='Positive', color='red', density=True)\n",
    "ax5.axvline(operating_thr, color='green', linestyle='--', linewidth=2, label=f'Threshold ({operating_thr:.2f})')\n",
    "ax5.set_xlabel(\"Decision Score\", fontsize=11)\n",
    "ax5.set_ylabel(\"Density\", fontsize=11)\n",
    "ax5.set_title(\"Score Distribution by Class\", fontsize=12, fontweight='bold')\n",
    "ax5.legend(loc=\"best\", fontsize=9)\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Confusion Matrix Heatmap\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "im = ax6.imshow(cm, cmap='Blues', aspect='auto')\n",
    "ax6.set_xticks([0, 1])\n",
    "ax6.set_yticks([0, 1])\n",
    "ax6.set_xticklabels(['Negative', 'Positive'])\n",
    "ax6.set_yticklabels(['Negative', 'Positive'])\n",
    "ax6.set_xlabel('Predicted', fontsize=11)\n",
    "ax6.set_ylabel('Actual', fontsize=11)\n",
    "ax6.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax6.text(j, i, f'{cm[i, j]}\\n({100*cm[i,j]/np.sum(cm):.1f}%)',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax6, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(f'{MODEL.upper()} - {task_name} - Comprehensive Metrics Dashboard', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "\n",
    "# Store figure reference for saving later\n",
    "comprehensive_dashboard_fig = fig\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Comprehensive metrics dashboard generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Analysis\n",
    "\n",
    "Explore how performance metrics change across different threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# THRESHOLD ANALYSIS TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"THRESHOLD SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Get thresholds from config\n",
    "rf_config = env.configs.randomforest\n",
    "threshold_config = rf_config.get('evaluation', {}).get('threshold_search', {})\n",
    "config_thresholds = threshold_config.get('thresholds', [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.40, 0.50])\n",
    "\n",
    "# Add the actual operating threshold to the list\n",
    "test_thresholds = sorted(set(config_thresholds + [operating_thr]))\n",
    "\n",
    "print(f\"\\n{'Threshold':^10} | {'Sens':^6} | {'Spec':^6} | {'PPV':^6} | {'NPV':^6} | {'F2':^6} | {'MCC':^6} | {'# Flag':^7} | {'NNS':^6}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for thr in test_thresholds:\n",
    "    y_pred_thr = (scores >= thr).astype(int)\n",
    "    cm_thr = confusion_matrix(y_true, y_pred_thr)\n",
    "    tn_t, fp_t, fn_t, tp_t = cm_thr.ravel()\n",
    "    \n",
    "    sens_t = tp_t / (tp_t + fn_t) if (tp_t + fn_t) > 0 else 0\n",
    "    spec_t = tn_t / (tn_t + fp_t) if (tn_t + fp_t) > 0 else 0\n",
    "    ppv_t = tp_t / (tp_t + fp_t) if (tp_t + fp_t) > 0 else 0\n",
    "    npv_t = tn_t / (tn_t + fn_t) if (tn_t + fn_t) > 0 else 0\n",
    "    \n",
    "    # F2 score (recall weighted 2x more than precision)\n",
    "    if ppv_t > 0 and sens_t > 0:\n",
    "        f2_t = 5 * ppv_t * sens_t / (4 * ppv_t + sens_t)\n",
    "    else:\n",
    "        f2_t = 0\n",
    "    \n",
    "    mcc_t = matthews_corrcoef(y_true, y_pred_thr)\n",
    "    nns_t = 1 / ppv_t if ppv_t > 0 else np.inf\n",
    "    n_flagged = y_pred_thr.sum()\n",
    "    \n",
    "    # Highlight current operating threshold (exact match)\n",
    "    marker = \" ← CURRENT\" if abs(thr - operating_thr) < 0.001 else \"\"\n",
    "    \n",
    "    print(f\"{thr:^10.3f} | {sens_t:^6.3f} | {spec_t:^6.3f} | {ppv_t:^6.3f} | {npv_t:^6.3f} | {f2_t:^6.3f} | {mcc_t:^6.3f} | {n_flagged:^7d} | {nns_t:^6.1f}{marker}\")\n",
    "\n",
    "print(f\"\\nNote: Operating threshold {operating_thr:.3f} is the mean of per-fold thresholds\")\n",
    "print(\"✓ Threshold analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRECISION-RECALL VS THRESHOLD VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Calculate metrics across threshold range\n",
    "threshold_range = np.linspace(0, 1, 100)\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "f1_scores = []\n",
    "f2_scores = []\n",
    "specificities = []\n",
    "\n",
    "for thr in threshold_range:\n",
    "    y_pred_t = (scores >= thr).astype(int)\n",
    "    cm_t = confusion_matrix(y_true, y_pred_t)\n",
    "    tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
    "    \n",
    "    recall_t = tp_t / (tp_t + fn_t) if (tp_t + fn_t) > 0 else 0\n",
    "    prec_t = tp_t / (tp_t + fp_t) if (tp_t + fp_t) > 0 else 0\n",
    "    spec_t = tn_t / (tn_t + fp_t) if (tn_t + fp_t) > 0 else 0\n",
    "    \n",
    "    if prec_t > 0 and recall_t > 0:\n",
    "        f1_t = 2 * prec_t * recall_t / (prec_t + recall_t)\n",
    "        f2_t = 5 * prec_t * recall_t / (4 * prec_t + recall_t)\n",
    "    else:\n",
    "        f1_t = 0\n",
    "        f2_t = 0\n",
    "    \n",
    "    precisions_list.append(prec_t)\n",
    "    recalls_list.append(recall_t)\n",
    "    f1_scores.append(f1_t)\n",
    "    f2_scores.append(f2_t)\n",
    "    specificities.append(spec_t)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left plot: Precision, Recall, Specificity vs Threshold\n",
    "ax1.plot(threshold_range, recalls_list, label='Recall (Sensitivity)', linewidth=2)\n",
    "ax1.plot(threshold_range, precisions_list, label='Precision (PPV)', linewidth=2)\n",
    "ax1.plot(threshold_range, specificities, label='Specificity', linewidth=2, linestyle='--')\n",
    "ax1.axvline(operating_thr, color='red', linestyle=':', linewidth=2, label=f'Operating ({operating_thr:.2f})')\n",
    "ax1.set_xlabel('Threshold', fontsize=12)\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_title('Precision, Recall, Specificity vs Threshold', fontsize=13, fontweight='bold')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Right plot: F1 and F2 scores vs Threshold\n",
    "ax2.plot(threshold_range, f1_scores, label='F1 Score', linewidth=2, color='green')\n",
    "ax2.plot(threshold_range, f2_scores, label='F2 Score (prioritizes recall)', linewidth=2, color='purple')\n",
    "ax2.axvline(operating_thr, color='red', linestyle=':', linewidth=2, label=f'Operating ({operating_thr:.2f})')\n",
    "\n",
    "# Mark best F2 threshold\n",
    "best_f2_idx = np.argmax(f2_scores)\n",
    "best_f2_thr = threshold_range[best_f2_idx]\n",
    "best_f2_val = f2_scores[best_f2_idx]\n",
    "ax2.scatter(best_f2_thr, best_f2_val, color='purple', s=150, zorder=5, marker='*',\n",
    "           label=f'Best F2 @ {best_f2_thr:.2f}')\n",
    "\n",
    "ax2.set_xlabel('Threshold', fontsize=12)\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_title('F-Scores vs Threshold', fontsize=13, fontweight='bold')\n",
    "ax2.legend(loc='best')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Store figure reference for saving later\n",
    "threshold_analysis_fig = fig\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Best F2 threshold: {best_f2_thr:.3f} (F2={best_f2_val:.3f})\")\n",
    "print(f\"  Current threshold: {operating_thr:.3f}\")\n",
    "print(f\"  Difference: {abs(best_f2_thr - operating_thr):.3f}\")\n",
    "print(\"\\n✓ Threshold analysis plots complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Confidence Predictions\n",
    "\n",
    "Examine the most confident predictions and how reliability varies by probability bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HIGH CONFIDENCE PREDICTIONS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HIGH CONFIDENCE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by confidence\n",
    "sorted_idx = np.argsort(scores)[::-1]\n",
    "sorted_scores = scores[sorted_idx]\n",
    "sorted_y_true = y_true[sorted_idx]\n",
    "sorted_y_pred = y_pred[sorted_idx]\n",
    "\n",
    "# Top confident predictions\n",
    "n_top = 20\n",
    "print(f\"\\n--- Top {n_top} Most Confident Predictions ---\")\n",
    "print(f\"{'Rank':^6} | {'Prob':^6} | {'Pred':^6} | {'True':^6} | {'Correct':^8}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(min(n_top, len(sorted_scores))):\n",
    "    prob = sorted_scores[i]\n",
    "    pred = \"POS\" if sorted_y_pred[i] == 1 else \"NEG\"\n",
    "    true = \"POS\" if sorted_y_true[i] == 1 else \"NEG\"\n",
    "    correct = \"✓\" if sorted_y_pred[i] == sorted_y_true[i] else \"✗\"\n",
    "    \n",
    "    print(f\"{i+1:^6d} | {prob:^6.3f} | {pred:^6s} | {true:^6s} | {correct:^8s}\")\n",
    "\n",
    "# Confidence bins\n",
    "print(f\"\\n--- Predictions by Confidence Bin ---\")\n",
    "confidence_bins = [(0.0, 0.05), (0.05, 0.10), (0.10, 0.15), (0.15, 0.20), (0.20, 0.50), (0.50, 1.0)]\n",
    "\n",
    "print(f\"{'Prob Range':^15} | {'Count':^7} | {'% True Pos':^11} | {'Actual PPV':^11}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for low, high in confidence_bins:\n",
    "    mask = (scores >= low) & (scores < high)\n",
    "    n_in_bin = mask.sum()\n",
    "    if n_in_bin > 0:\n",
    "        n_actual_pos = y_true[mask].sum()\n",
    "        pct_actual_pos = 100 * n_actual_pos / n_in_bin\n",
    "        \n",
    "        # PPV in this bin (among those predicted positive)\n",
    "        pred_pos_mask = mask & (y_pred == 1)\n",
    "        if pred_pos_mask.sum() > 0:\n",
    "            ppv = y_true[pred_pos_mask].sum() / pred_pos_mask.sum()\n",
    "        else:\n",
    "            ppv = 0.0\n",
    "        \n",
    "        print(f\"[{low:.2f}, {high:.2f})   | {n_in_bin:^7d} | {pct_actual_pos:^11.1f} | {ppv:^11.3f}\")\n",
    "\n",
    "# Summary stats for high confidence positives\n",
    "high_conf_mask = (scores >= 0.15) & (y_pred == 1)\n",
    "if high_conf_mask.sum() > 0:\n",
    "    high_conf_ppv = y_true[high_conf_mask].sum() / high_conf_mask.sum()\n",
    "    print(f\"\\n✓ High confidence (≥0.15) predictions: {high_conf_mask.sum()}\")\n",
    "    print(f\"  Precision among high-confidence: {high_conf_ppv:.3f}\")\n",
    "    print(f\"  Catching {y_true[high_conf_mask].sum()} of {y_true.sum()} true positives ({100*y_true[high_conf_mask].sum()/y_true.sum():.1f}%)\")\n",
    "\n",
    "print(\"\\n✓ High confidence analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ERROR ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# False Positives (predicted positive, actually negative)\n",
    "fp_mask = (y_pred == 1) & (y_true == 0)\n",
    "fp_scores = scores[fp_mask]\n",
    "\n",
    "# False Negatives (predicted negative, actually positive)\n",
    "fn_mask = (y_pred == 0) & (y_true == 1)\n",
    "fn_scores = scores[fn_mask]\n",
    "\n",
    "# True Positives\n",
    "tp_mask = (y_pred == 1) & (y_true == 1)\n",
    "tp_scores = scores[tp_mask]\n",
    "\n",
    "# True Negatives\n",
    "tn_mask = (y_pred == 0) & (y_true == 0)\n",
    "tn_scores = scores[tn_mask]\n",
    "\n",
    "print(f\"\\n--- Error Distribution ---\")\n",
    "print(f\"False Positives: {fp_mask.sum()} (most confident wrong alarms)\")\n",
    "if fp_mask.sum() > 0:\n",
    "    print(f\"  Score range: [{fp_scores.min():.3f}, {fp_scores.max():.3f}]\")\n",
    "    print(f\"  Mean score: {fp_scores.mean():.3f}\")\n",
    "    print(f\"  Median score: {np.median(fp_scores):.3f}\")\n",
    "    \n",
    "    # Show most confident false positives\n",
    "    fp_indices = np.where(fp_mask)[0]\n",
    "    fp_sorted_idx = fp_indices[np.argsort(scores[fp_indices])[::-1]]\n",
    "    \n",
    "    print(f\"\\n  Top 10 Most Confident False Positives:\")\n",
    "    print(f\"  {'Index':^8} | {'Score':^8}\")\n",
    "    print(\"  \" + \"-\" * 20)\n",
    "    for idx in fp_sorted_idx[:10]:\n",
    "        print(f\"  {idx:^8d} | {scores[idx]:^8.3f}\")\n",
    "\n",
    "print(f\"\\nFalse Negatives: {fn_mask.sum()} (missed cases)\")\n",
    "if fn_mask.sum() > 0:\n",
    "    print(f\"  Score range: [{fn_scores.min():.3f}, {fn_scores.max():.3f}]\")\n",
    "    print(f\"  Mean score: {fn_scores.mean():.3f}\")\n",
    "    print(f\"  Median score: {np.median(fn_scores):.3f}\")\n",
    "    \n",
    "    # Show closest misses (highest scoring false negatives)\n",
    "    fn_indices = np.where(fn_mask)[0]\n",
    "    fn_sorted_idx = fn_indices[np.argsort(scores[fn_indices])[::-1]]\n",
    "    \n",
    "    print(f\"\\n  Top 10 False Negatives (closest to threshold):\")\n",
    "    print(f\"  {'Index':^8} | {'Score':^8} | {'Gap to Thr':^12}\")\n",
    "    print(\"  \" + \"-\" * 32)\n",
    "    for idx in fn_sorted_idx[:10]:\n",
    "        gap = operating_thr - scores[idx]\n",
    "        print(f\"  {idx:^8d} | {scores[idx]:^8.3f} | {gap:^12.3f}\")\n",
    "\n",
    "print(f\"\\n--- Success Distribution ---\")\n",
    "print(f\"True Positives: {tp_mask.sum()}\")\n",
    "if tp_mask.sum() > 0:\n",
    "    print(f\"  Mean score: {tp_scores.mean():.3f} (avg confidence when correct)\")\n",
    "    print(f\"  Min score: {tp_scores.min():.3f} (barely caught)\")\n",
    "    print(f\"  Max score: {tp_scores.max():.3f} (highest confidence)\")\n",
    "\n",
    "print(f\"\\nTrue Negatives: {tn_mask.sum()}\")\n",
    "if tn_mask.sum() > 0:\n",
    "    print(f\"  Mean score: {tn_scores.mean():.3f}\")\n",
    "    print(f\"  Max score: {tn_scores.max():.3f} (closest call to threshold)\")\n",
    "\n",
    "# Score separation analysis\n",
    "print(f\"\\n--- Class Separation ---\")\n",
    "pos_scores = scores[y_true == 1]\n",
    "neg_scores = scores[y_true == 0]\n",
    "print(f\"Positive class mean: {pos_scores.mean():.3f} ± {pos_scores.std():.3f}\")\n",
    "print(f\"Negative class mean: {neg_scores.mean():.3f} ± {neg_scores.std():.3f}\")\n",
    "print(f\"Mean difference: {pos_scores.mean() - neg_scores.mean():.3f}\")\n",
    "\n",
    "print(\"\\n✓ Error analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Comprehensive Run Results\n",
    "\n",
    "Save all results, configs, and figures for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"svm\"\n",
    "TASK_NAME = task_config['name']\n",
    "\n",
    "# Create timestamped save directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir = (env.repo_root / \"outputs\" / run_cfg['run_name'] / run_cfg['run_id'] /\n",
    "            f\"seed_{seed}\" / MODEL_NAME / TASK_NAME / f\"analysis_{timestamp}\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING COMPREHENSIVE RUN RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSave directory: {save_dir}\")\n",
    "\n",
    "# 1. Copy results pickle\n",
    "results_path = (env.repo_root / \"outputs\" / run_cfg['run_name'] / run_cfg['run_id'] /\n",
    "                f\"seed_{seed}\" / MODEL_NAME / TASK_NAME / \"results.pkl\")\n",
    "if results_path.exists():\n",
    "    shutil.copy(results_path, save_dir / \"results.pkl\")\n",
    "    print(f\"✓ Copied results.pkl\")\n",
    "\n",
    "# 2. Save configurations\n",
    "configs_dir = save_dir / \"configs\"\n",
    "configs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import yaml\n",
    "with open(configs_dir / f\"{MODEL_NAME}.yaml\", \"w\") as f:\n",
    "    yaml.dump(dict(getattr(env.configs, MODEL_NAME)), f, default_flow_style=False)\n",
    "with open(configs_dir / \"run.yaml\", \"w\") as f:\n",
    "    yaml.dump(dict(env.configs.run), f, default_flow_style=False)\n",
    "with open(configs_dir / \"task.json\", \"w\") as f:\n",
    "    json.dump(task_config, f, indent=2)\n",
    "print(f\"✓ Saved config files\")\n",
    "\n",
    "# 3. Save all matplotlib figures\n",
    "figures_dir = save_dir / \"figures\"\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "saved_figs = []\n",
    "\n",
    "# Save the comprehensive dashboard figure (from cell 11)\n",
    "try:\n",
    "    comprehensive_dashboard_fig.savefig(\n",
    "        figures_dir / \"comprehensive_metrics_dashboard.png\", \n",
    "        dpi=300, bbox_inches='tight'\n",
    "    )\n",
    "    saved_figs.append(\"comprehensive_metrics_dashboard.png\")\n",
    "except NameError:\n",
    "    print(\"  ⚠ Warning: comprehensive_dashboard_fig not found - run cell 11 first\")\n",
    "\n",
    "# Save the threshold analysis figure (from cell 14)\n",
    "try:\n",
    "    threshold_analysis_fig.savefig(\n",
    "        figures_dir / \"threshold_analysis.png\", \n",
    "        dpi=300, bbox_inches='tight'\n",
    "    )\n",
    "    saved_figs.append(\"threshold_analysis.png\")\n",
    "except NameError:\n",
    "    print(\"  ⚠ Warning: threshold_analysis_fig not found - run cell 14 first\")\n",
    "\n",
    "# Also save any other active figures\n",
    "for i in plt.get_fignums():\n",
    "    fig = plt.figure(i)\n",
    "    fig.savefig(figures_dir / f\"figure_{i:02d}.png\", dpi=300, bbox_inches='tight')\n",
    "    saved_figs.append(f\"figure_{i:02d}.png\")\n",
    "\n",
    "# Copy confusion matrices from pipeline output\n",
    "plots_dir = (env.repo_root / \"outputs\" / run_cfg['run_name'] / run_cfg['run_id'] /\n",
    "             f\"seed_{seed}\" / MODEL_NAME / TASK_NAME / \"plots\")\n",
    "if plots_dir.exists():\n",
    "    for plot_file in plots_dir.glob(\"*.png\"):\n",
    "        shutil.copy(plot_file, figures_dir / plot_file.name)\n",
    "        saved_figs.append(plot_file.name)\n",
    "\n",
    "if saved_figs:\n",
    "    print(f\"✓ Saved {len(saved_figs)} figures:\")\n",
    "    for fig_name in saved_figs:\n",
    "        print(f\"  - {fig_name}\")\n",
    "else:\n",
    "    print(\"⚠ No figures saved - make sure to run the analysis cells first\")\n",
    "\n",
    "# 4. Save metrics summary\n",
    "metrics_summary = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"task\": TASK_NAME,\n",
    "    \"run_id\": run_cfg['run_id'],\n",
    "    \"seed\": seed,\n",
    "    \"overall_metrics\": {\n",
    "        \"baseline\": {k: float(v) if isinstance(v, (int, float, np.number)) else v\n",
    "                     for k, v in results['baseline']['overall'].items()},\n",
    "        MODEL_NAME: {k: float(v) if isinstance(v, (int, float, np.number)) else v\n",
    "                     for k, v in results[MODEL_NAME]['overall'].items()}\n",
    "    },\n",
    "    \"per_fold_stats\": {\n",
    "        k: float(v) if isinstance(v, (int, float, np.number)) else v\n",
    "        for k, v in results[MODEL_NAME]['per_fold'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(save_dir / \"metrics_summary.json\", \"w\") as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "# 5. Save comprehensive metrics (if computed)\n",
    "if 'roc_auc' in locals():\n",
    "    comprehensive = {\n",
    "        \"discrimination\": {\n",
    "            \"roc_auc\": float(roc_auc),\n",
    "            \"pr_auc\": float(pr_auc),\n",
    "            \"mcc\": float(mcc)\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"sensitivity\": float(sensitivity),\n",
    "            \"specificity\": float(specificity),\n",
    "            \"precision\": float(precision),\n",
    "            \"npv\": float(npv)\n",
    "        },\n",
    "        \"errors\": {\n",
    "            \"fpr\": float(fpr),\n",
    "            \"fnr\": float(fnr),\n",
    "            \"fdr\": float(fdr)\n",
    "        },\n",
    "        \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "        \"threshold\": float(operating_thr)\n",
    "    }\n",
    "    with open(save_dir / \"comprehensive_metrics.json\", \"w\") as f:\n",
    "        json.dump(comprehensive, f, indent=2)\n",
    "    print(f\"✓ Saved comprehensive metrics\")\n",
    "\n",
    "# 6. Create README\n",
    "readme = f\"\"\"# {MODEL_NAME.upper()} Results: {TASK_NAME}\n",
    "\n",
    "**Timestamp**: {timestamp}\n",
    "**Run ID**: {run_cfg['run_id']}\n",
    "**Seed**: {seed}\n",
    "\n",
    "## Performance\n",
    "- ROC-AUC: {results[MODEL_NAME]['overall']['roc_auc']:.3f}\n",
    "- Balanced Accuracy: {results[MODEL_NAME]['overall']['balanced_accuracy']:.3f}\n",
    "\n",
    "## Files\n",
    "- `results.pkl`: Complete results\n",
    "- `metrics_summary.json`: Key metrics\n",
    "- `comprehensive_metrics.json`: Detailed metrics\n",
    "- `configs/`: Configuration files\n",
    "- `figures/`: All visualizations\n",
    "\n",
    "## Reproducibility\n",
    "Use configs in `configs/` with seed {seed}\n",
    "\"\"\"\n",
    "\n",
    "with open(save_dir / \"README.md\", \"w\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(f\"\\n✓ All results saved to: {save_dir.name}\")\n",
    "print(f\"✓ Total files: {len(list(save_dir.rglob('*')))}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Tasks (Optional)\n",
    "\n",
    "Once single task works, you can run all tasks at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.svm.pipeline import run_svm_pipeline\n",
    "\n",
    "# Run complete pipeline for all tasks\n",
    "all_results = run_svm_pipeline(env, use_wandb=False, sweep_mode=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
